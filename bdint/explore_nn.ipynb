{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 1460\n",
      "Test Set Size: 1459\n",
      "Epoch: 0, Loss: 197194.265625\n",
      "Epoch: 1, Loss: 197158.609375\n",
      "Epoch: 2, Loss: 197080.71875\n",
      "Epoch: 3, Loss: 196936.3125\n",
      "Epoch: 4, Loss: 196692.765625\n",
      "Epoch: 5, Loss: 196324.015625\n",
      "Epoch: 6, Loss: 195833.09375\n",
      "Epoch: 7, Loss: 195280.453125\n",
      "Epoch: 8, Loss: 194820.03125\n",
      "Epoch: 9, Loss: 193524.125\n",
      "Epoch: 10, Loss: 192365.515625\n",
      "Epoch: 11, Loss: 191081.796875\n",
      "Epoch: 12, Loss: 189609.203125\n",
      "Epoch: 13, Loss: 187980.265625\n",
      "Epoch: 14, Loss: 186273.15625\n",
      "Epoch: 15, Loss: 184452.703125\n",
      "Epoch: 16, Loss: 181915.203125\n",
      "Epoch: 17, Loss: 179306.296875\n",
      "Epoch: 18, Loss: 176525.484375\n",
      "Epoch: 19, Loss: 173760.328125\n",
      "Epoch: 20, Loss: 171096.40625\n",
      "Epoch: 21, Loss: 166950.796875\n",
      "Epoch: 22, Loss: 163309.796875\n",
      "Epoch: 23, Loss: 159102.265625\n",
      "Epoch: 24, Loss: 155092.296875\n",
      "Epoch: 25, Loss: 150826.59375\n",
      "Epoch: 26, Loss: 145669.25\n",
      "Epoch: 27, Loss: 140490.96875\n",
      "Epoch: 28, Loss: 135612.046875\n",
      "Epoch: 29, Loss: 130374.828125\n",
      "Epoch: 30, Loss: 123725.046875\n",
      "Epoch: 31, Loss: 118007.5625\n",
      "Epoch: 32, Loss: 112956.2734375\n",
      "Epoch: 33, Loss: 106050.40625\n",
      "Epoch: 34, Loss: 98864.5625\n",
      "Epoch: 35, Loss: 92320.2578125\n",
      "Epoch: 36, Loss: 86659.765625\n",
      "Epoch: 37, Loss: 84371.03125\n",
      "Epoch: 38, Loss: 72908.8203125\n",
      "Epoch: 39, Loss: 69689.3125\n",
      "Epoch: 40, Loss: 60843.7734375\n",
      "Epoch: 41, Loss: 55188.31640625\n",
      "Epoch: 42, Loss: 49683.2890625\n",
      "Epoch: 43, Loss: 45502.21484375\n",
      "Epoch: 44, Loss: 43829.63671875\n",
      "Epoch: 45, Loss: 50173.89453125\n",
      "Epoch: 46, Loss: 52165.46484375\n",
      "Epoch: 47, Loss: 44814.125\n",
      "Epoch: 48, Loss: 51823.62890625\n",
      "Epoch: 49, Loss: 47947.33203125\n",
      "Epoch: 50, Loss: 48400.86328125\n",
      "Epoch: 51, Loss: 47353.44921875\n",
      "Epoch: 52, Loss: 46516.8125\n",
      "Epoch: 53, Loss: 44058.078125\n",
      "Epoch: 54, Loss: 44838.59765625\n",
      "Epoch: 55, Loss: 43705.31640625\n",
      "Epoch: 56, Loss: 41734.06640625\n",
      "Epoch: 57, Loss: 40515.46875\n",
      "Epoch: 58, Loss: 40058.5625\n",
      "Epoch: 59, Loss: 39466.73828125\n",
      "Epoch: 60, Loss: 39280.72265625\n",
      "Epoch: 61, Loss: 40969.4296875\n",
      "Epoch: 62, Loss: 46630.0390625\n",
      "Epoch: 63, Loss: 41207.54296875\n",
      "Epoch: 64, Loss: 41247.0\n",
      "Epoch: 65, Loss: 39912.55859375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m\n\u001b[1;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m NN(hidden_size1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1028\u001b[39m, hidden_size2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, hidden_size3\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# numerical\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# train_df = preprocess_for_numerical_model(train_df)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# test_df = preprocess_for_numerical_model(test_df)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Check Performance of model using k validation\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m rmses \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mk_fold_validation\u001b[0;34m(train_df, model, k)\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Learn the given model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculate RMSE\u001b[39;00m\n\u001b[1;32m     27\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "File \u001b[0;32m~/bdint/80240793-Big-Data-Intelligence/bdint/models/nn.py:69\u001b[0m, in \u001b[0;36mNN.learn\u001b[0;34m(self, x_train_df, y_train_df, epochs, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m     66\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 69\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(criterion(y_pred, y_train_tensor))\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/bdint/80240793-Big-Data-Intelligence/bdint/models/nn.py:52\u001b[0m, in \u001b[0;36mNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m     51\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(out)\n\u001b[0;32m---> 52\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     54\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from bdint.models.nn import NN\n",
    "\n",
    "import torch\n",
    "\n",
    "def k_fold_validation(train_df, model, k=5):\n",
    "    target = train_df[[\"SalePrice\"]]\n",
    "    features = train_df.drop(columns=[\"SalePrice\"])\n",
    "    target.columns = [\"SalePrice\"]\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    rmse_values = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        x_train, x_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "        \n",
    "        # reset model\n",
    "        model.reset()\n",
    "\n",
    "        # Learn the given model\n",
    "        model.learn(x_train, y_train, x_test, y_test, epochs=500, learning_rate=0.5, weight_decay=0.1)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        predictions = model.predict(x_test)\n",
    "        if isinstance(predictions, torch.Tensor):\n",
    "            predictions = predictions.detach().numpy()\n",
    "        rmse_value = mean_squared_error(y_test[\"SalePrice\"], predictions, squared=False)\n",
    "        rmse_values.append(rmse_value)\n",
    "\n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = sum(rmse_values) / len(rmse_values)\n",
    "\n",
    "    return mean_rmse\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bdint.data import (\n",
    "    get_test_df,\n",
    "    get_train_df,\n",
    "    make_kaggle_submission_file,\n",
    ")\n",
    "from models import CatBoost, RandomForest\n",
    "\n",
    "train_df = get_train_df(\"../data/train.csv\")\n",
    "test_df = get_test_df(\"../data/test.csv\")\n",
    "\n",
    "print(\"Train Set Size:\", len(train_df))\n",
    "print(\"Test Set Size:\", len(test_df))\n",
    "\n",
    "# create Model\n",
    "# model = RandomForest(n_estimators=100, random_state=42)\n",
    "# set jobtype to cpu\n",
    "# model = CatBoost(early_stopping_rounds=2000, iterations=10000)\n",
    "# model = CatBoost(early_stopping_rounds=2000, iterations=15000)\n",
    "model = NN(hidden_size1=1028, hidden_size2=512, hidden_size3=256)\n",
    "\n",
    "# numerical\n",
    "# train_df = preprocess_for_numerical_model(train_df)\n",
    "# test_df = preprocess_for_numerical_model(test_df)\n",
    "\n",
    "# Check Performance of model using k validation\n",
    "rmses = k_fold_validation(train_df=train_df, model=model, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 47087.4132771151\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean RMSE:\", rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 197583.671875\n",
      "Epoch: 1, Loss: 197549.953125\n",
      "Epoch: 2, Loss: 197478.875\n",
      "Epoch: 3, Loss: 197345.875\n",
      "Epoch: 4, Loss: 197121.359375\n",
      "Epoch: 5, Loss: 196783.03125\n",
      "Epoch: 6, Loss: 196299.4375\n",
      "Epoch: 7, Loss: 195776.171875\n",
      "Epoch: 8, Loss: 195187.671875\n",
      "Epoch: 9, Loss: 194044.6875\n",
      "Epoch: 10, Loss: 192963.15625\n",
      "Epoch: 11, Loss: 191703.984375\n",
      "Epoch: 12, Loss: 190265.484375\n",
      "Epoch: 13, Loss: 188683.140625\n",
      "Epoch: 14, Loss: 187345.359375\n",
      "Epoch: 15, Loss: 184995.6875\n",
      "Epoch: 16, Loss: 182503.578125\n",
      "Epoch: 17, Loss: 180126.390625\n",
      "Epoch: 18, Loss: 177617.953125\n",
      "Epoch: 19, Loss: 174410.828125\n",
      "Epoch: 20, Loss: 171262.03125\n",
      "Epoch: 21, Loss: 167680.9375\n",
      "Epoch: 22, Loss: 164376.359375\n",
      "Epoch: 23, Loss: 160079.640625\n",
      "Epoch: 24, Loss: 155643.28125\n",
      "Epoch: 25, Loss: 151103.046875\n",
      "Epoch: 26, Loss: 146280.125\n",
      "Epoch: 27, Loss: 142783.25\n",
      "Epoch: 28, Loss: 138014.046875\n",
      "Epoch: 29, Loss: 131710.265625\n",
      "Epoch: 30, Loss: 125203.4453125\n",
      "Epoch: 31, Loss: 119981.71875\n",
      "Epoch: 32, Loss: 113421.7734375\n",
      "Epoch: 33, Loss: 107586.734375\n",
      "Epoch: 34, Loss: 100603.28125\n",
      "Epoch: 35, Loss: 93239.328125\n",
      "Epoch: 36, Loss: 86149.546875\n",
      "Epoch: 37, Loss: 79763.7265625\n",
      "Epoch: 38, Loss: 74217.1171875\n",
      "Epoch: 39, Loss: 70556.328125\n",
      "Epoch: 40, Loss: 60797.4296875\n",
      "Epoch: 41, Loss: 54865.78125\n",
      "Epoch: 42, Loss: 50719.72265625\n",
      "Epoch: 43, Loss: 45305.8671875\n",
      "Epoch: 44, Loss: 45030.93359375\n",
      "Epoch: 45, Loss: 42494.56640625\n",
      "Epoch: 46, Loss: 44425.39453125\n",
      "Epoch: 47, Loss: 48206.734375\n",
      "Epoch: 48, Loss: 45410.12109375\n",
      "Epoch: 49, Loss: 47301.78515625\n",
      "Epoch: 50, Loss: 44481.4453125\n",
      "Epoch: 51, Loss: 37750.74609375\n",
      "Epoch: 52, Loss: 38671.04296875\n",
      "Epoch: 53, Loss: 38422.1953125\n",
      "Epoch: 54, Loss: 36435.875\n",
      "Epoch: 55, Loss: 34762.08203125\n",
      "Epoch: 56, Loss: 33620.86328125\n",
      "Epoch: 57, Loss: 33228.921875\n",
      "Epoch: 58, Loss: 34035.52734375\n",
      "Epoch: 59, Loss: 31705.10546875\n",
      "Epoch: 60, Loss: 34399.90234375\n",
      "Epoch: 61, Loss: 37870.33984375\n",
      "Epoch: 62, Loss: 36504.78125\n",
      "Epoch: 63, Loss: 33818.3046875\n",
      "Epoch: 64, Loss: 33731.8046875\n",
      "Epoch: 65, Loss: 32003.25390625\n",
      "Epoch: 66, Loss: 31550.02734375\n",
      "Epoch: 67, Loss: 32048.30859375\n",
      "Epoch: 68, Loss: 32788.4296875\n",
      "Epoch: 69, Loss: 32306.970703125\n",
      "Epoch: 70, Loss: 29195.974609375\n",
      "Epoch: 71, Loss: 30501.58984375\n",
      "Epoch: 72, Loss: 30168.958984375\n",
      "Epoch: 73, Loss: 28904.984375\n",
      "Epoch: 74, Loss: 28281.412109375\n",
      "Epoch: 75, Loss: 30597.349609375\n",
      "Epoch: 76, Loss: 27504.826171875\n",
      "Epoch: 77, Loss: 28881.404296875\n",
      "Epoch: 78, Loss: 26969.904296875\n",
      "Epoch: 79, Loss: 28601.982421875\n",
      "Epoch: 80, Loss: 27912.677734375\n",
      "Epoch: 81, Loss: 32419.955078125\n",
      "Epoch: 82, Loss: 35849.56640625\n",
      "Epoch: 83, Loss: 33492.76171875\n",
      "Epoch: 84, Loss: 31604.9296875\n",
      "Epoch: 85, Loss: 32333.478515625\n",
      "Epoch: 86, Loss: 30815.0\n",
      "Epoch: 87, Loss: 29487.92578125\n",
      "Epoch: 88, Loss: 27360.759765625\n",
      "Epoch: 89, Loss: 28315.8046875\n",
      "Epoch: 90, Loss: 29112.904296875\n",
      "Epoch: 91, Loss: 29898.59765625\n",
      "Epoch: 92, Loss: 28243.078125\n",
      "Epoch: 93, Loss: 28173.89453125\n",
      "Epoch: 94, Loss: 27636.08984375\n",
      "Epoch: 95, Loss: 26961.373046875\n",
      "Epoch: 96, Loss: 26597.58984375\n",
      "Epoch: 97, Loss: 27292.482421875\n",
      "Epoch: 98, Loss: 26330.63671875\n",
      "Epoch: 99, Loss: 26648.666015625\n",
      "Epoch: 100, Loss: 28590.31640625\n",
      "Epoch: 101, Loss: 26685.345703125\n",
      "Epoch: 102, Loss: 26653.966796875\n",
      "Epoch: 103, Loss: 28006.7421875\n",
      "Epoch: 104, Loss: 27553.931640625\n",
      "Epoch: 105, Loss: 26021.927734375\n",
      "Epoch: 106, Loss: 28073.115234375\n",
      "Epoch: 107, Loss: 27347.486328125\n",
      "Epoch: 108, Loss: 26765.740234375\n",
      "Epoch: 109, Loss: 27086.064453125\n",
      "Epoch: 110, Loss: 25959.478515625\n",
      "Epoch: 111, Loss: 26467.009765625\n",
      "Epoch: 112, Loss: 25482.623046875\n",
      "Epoch: 113, Loss: 26647.20703125\n",
      "Epoch: 114, Loss: 25306.833984375\n",
      "Epoch: 115, Loss: 24984.255859375\n",
      "Epoch: 116, Loss: 26067.302734375\n",
      "Epoch: 117, Loss: 25589.90234375\n",
      "Epoch: 118, Loss: 25882.859375\n",
      "Epoch: 119, Loss: 27220.224609375\n",
      "Epoch: 120, Loss: 27505.48046875\n",
      "Epoch: 121, Loss: 27297.353515625\n",
      "Epoch: 122, Loss: 27300.2734375\n",
      "Epoch: 123, Loss: 26252.455078125\n",
      "Epoch: 124, Loss: 26485.109375\n",
      "Epoch: 125, Loss: 26384.017578125\n",
      "Epoch: 126, Loss: 27679.984375\n",
      "Epoch: 127, Loss: 26652.451171875\n",
      "Epoch: 128, Loss: 25526.501953125\n",
      "Epoch: 129, Loss: 27429.609375\n",
      "Epoch: 130, Loss: 26581.8359375\n",
      "Epoch: 131, Loss: 26609.9296875\n",
      "Epoch: 132, Loss: 26468.369140625\n",
      "Epoch: 133, Loss: 26544.9140625\n",
      "Epoch: 134, Loss: 27244.1640625\n",
      "Epoch: 135, Loss: 27290.388671875\n",
      "Epoch: 136, Loss: 25981.990234375\n",
      "Epoch: 137, Loss: 27891.51171875\n",
      "Epoch: 138, Loss: 27188.658203125\n",
      "Epoch: 139, Loss: 27638.603515625\n",
      "Epoch: 140, Loss: 27727.29296875\n",
      "Epoch: 141, Loss: 26310.20703125\n",
      "Epoch: 142, Loss: 26805.5703125\n",
      "Epoch: 143, Loss: 26135.486328125\n",
      "Epoch: 144, Loss: 26899.900390625\n",
      "Epoch: 145, Loss: 26598.69921875\n",
      "Epoch: 146, Loss: 26815.36328125\n",
      "Epoch: 147, Loss: 29050.205078125\n",
      "Epoch: 148, Loss: 25902.537109375\n",
      "Epoch: 149, Loss: 26169.54296875\n",
      "Epoch: 150, Loss: 26010.04296875\n",
      "Epoch: 151, Loss: 25439.029296875\n",
      "Epoch: 152, Loss: 25309.53515625\n",
      "Epoch: 153, Loss: 25498.81640625\n",
      "Epoch: 154, Loss: 24912.69140625\n",
      "Epoch: 155, Loss: 25207.8515625\n",
      "Epoch: 156, Loss: 24752.330078125\n",
      "Epoch: 157, Loss: 25357.794921875\n",
      "Epoch: 158, Loss: 27500.697265625\n",
      "Epoch: 159, Loss: 25723.1015625\n",
      "Epoch: 160, Loss: 26270.03125\n",
      "Epoch: 161, Loss: 26737.259765625\n",
      "Epoch: 162, Loss: 27449.0\n",
      "Epoch: 163, Loss: 27065.423828125\n",
      "Epoch: 164, Loss: 25528.615234375\n",
      "Epoch: 165, Loss: 26693.333984375\n",
      "Epoch: 166, Loss: 25233.1171875\n",
      "Epoch: 167, Loss: 27483.818359375\n",
      "Epoch: 168, Loss: 26088.935546875\n",
      "Epoch: 169, Loss: 25194.646484375\n",
      "Epoch: 170, Loss: 24878.16796875\n",
      "Epoch: 171, Loss: 24681.94140625\n",
      "Epoch: 172, Loss: 24541.044921875\n",
      "Epoch: 173, Loss: 24918.23046875\n",
      "Epoch: 174, Loss: 25466.78125\n",
      "Epoch: 175, Loss: 25317.447265625\n",
      "Epoch: 176, Loss: 25638.513671875\n",
      "Epoch: 177, Loss: 26756.70703125\n",
      "Epoch: 178, Loss: 25523.703125\n",
      "Epoch: 179, Loss: 27395.08203125\n",
      "Epoch: 180, Loss: 26364.28125\n",
      "Epoch: 181, Loss: 25324.6484375\n",
      "Epoch: 182, Loss: 24744.375\n",
      "Epoch: 183, Loss: 25587.681640625\n",
      "Epoch: 184, Loss: 25514.67578125\n",
      "Epoch: 185, Loss: 29691.103515625\n",
      "Epoch: 186, Loss: 26031.4453125\n",
      "Epoch: 187, Loss: 26196.310546875\n",
      "Epoch: 188, Loss: 24480.56640625\n",
      "Epoch: 189, Loss: 26374.328125\n",
      "Epoch: 190, Loss: 25484.978515625\n",
      "Epoch: 191, Loss: 25550.974609375\n",
      "Epoch: 192, Loss: 25165.583984375\n",
      "Epoch: 193, Loss: 25014.466796875\n",
      "Epoch: 194, Loss: 24574.685546875\n",
      "Epoch: 195, Loss: 24881.787109375\n",
      "Epoch: 196, Loss: 24923.26953125\n",
      "Epoch: 197, Loss: 25592.67578125\n",
      "Epoch: 198, Loss: 24740.923828125\n",
      "Epoch: 199, Loss: 24286.21484375\n",
      "Epoch: 200, Loss: 25572.00390625\n",
      "Epoch: 201, Loss: 25177.19921875\n",
      "Epoch: 202, Loss: 26045.25\n",
      "Epoch: 203, Loss: 23950.310546875\n",
      "Epoch: 204, Loss: 25784.392578125\n",
      "Epoch: 205, Loss: 25201.654296875\n",
      "Epoch: 206, Loss: 24667.240234375\n",
      "Epoch: 207, Loss: 24483.55078125\n",
      "Epoch: 208, Loss: 24921.576171875\n",
      "Epoch: 209, Loss: 23876.8515625\n",
      "Epoch: 210, Loss: 26338.97265625\n",
      "Epoch: 211, Loss: 26873.041015625\n",
      "Epoch: 212, Loss: 25329.068359375\n",
      "Epoch: 213, Loss: 25321.943359375\n",
      "Epoch: 214, Loss: 25119.396484375\n",
      "Epoch: 215, Loss: 25155.443359375\n",
      "Epoch: 216, Loss: 25830.658203125\n",
      "Epoch: 217, Loss: 24361.36328125\n",
      "Epoch: 218, Loss: 24398.9140625\n",
      "Epoch: 219, Loss: 24444.40234375\n",
      "Epoch: 220, Loss: 24712.685546875\n",
      "Epoch: 221, Loss: 26469.408203125\n",
      "Epoch: 222, Loss: 24955.443359375\n",
      "Epoch: 223, Loss: 25208.94921875\n",
      "Epoch: 224, Loss: 25762.158203125\n",
      "Epoch: 225, Loss: 25719.361328125\n",
      "Epoch: 226, Loss: 25892.09765625\n",
      "Epoch: 227, Loss: 25011.51171875\n",
      "Epoch: 228, Loss: 26033.537109375\n",
      "Epoch: 229, Loss: 25204.0546875\n",
      "Epoch: 230, Loss: 25982.248046875\n",
      "Epoch: 231, Loss: 26196.69140625\n",
      "Epoch: 232, Loss: 26133.64453125\n",
      "Epoch: 233, Loss: 24633.86328125\n",
      "Epoch: 234, Loss: 25135.876953125\n",
      "Epoch: 235, Loss: 25572.306640625\n",
      "Epoch: 236, Loss: 27043.365234375\n",
      "Epoch: 237, Loss: 25244.416015625\n",
      "Epoch: 238, Loss: 25055.64453125\n",
      "Epoch: 239, Loss: 24292.28515625\n",
      "Epoch: 240, Loss: 25052.583984375\n",
      "Epoch: 241, Loss: 26657.25\n",
      "Epoch: 242, Loss: 25257.27734375\n",
      "Epoch: 243, Loss: 26609.642578125\n",
      "Epoch: 244, Loss: 24942.064453125\n",
      "Epoch: 245, Loss: 25078.162109375\n",
      "Epoch: 246, Loss: 24024.615234375\n",
      "Epoch: 247, Loss: 25780.802734375\n",
      "Epoch: 248, Loss: 27167.505859375\n",
      "Epoch: 249, Loss: 24690.548828125\n",
      "Epoch: 250, Loss: 26567.181640625\n",
      "Epoch: 251, Loss: 25759.3828125\n",
      "Epoch: 252, Loss: 25173.947265625\n",
      "Epoch: 253, Loss: 24418.669921875\n",
      "Epoch: 254, Loss: 23621.404296875\n",
      "Epoch: 255, Loss: 23819.90234375\n",
      "Epoch: 256, Loss: 24622.0390625\n",
      "Epoch: 257, Loss: 24695.61328125\n",
      "Epoch: 258, Loss: 22959.19140625\n",
      "Epoch: 259, Loss: 24194.091796875\n",
      "Epoch: 260, Loss: 24243.62890625\n",
      "Epoch: 261, Loss: 24336.904296875\n",
      "Epoch: 262, Loss: 24337.8046875\n",
      "Epoch: 263, Loss: 24989.802734375\n",
      "Epoch: 264, Loss: 24463.412109375\n",
      "Epoch: 265, Loss: 23550.595703125\n",
      "Epoch: 266, Loss: 23156.455078125\n",
      "Epoch: 267, Loss: 25185.685546875\n",
      "Epoch: 268, Loss: 23653.392578125\n",
      "Epoch: 269, Loss: 26191.265625\n",
      "Epoch: 270, Loss: 25855.59375\n",
      "Epoch: 271, Loss: 24834.212890625\n",
      "Epoch: 272, Loss: 24176.322265625\n",
      "Epoch: 273, Loss: 25278.650390625\n",
      "Epoch: 274, Loss: 23588.57421875\n",
      "Epoch: 275, Loss: 24177.748046875\n",
      "Epoch: 276, Loss: 24801.3984375\n",
      "Epoch: 277, Loss: 25247.390625\n",
      "Epoch: 278, Loss: 24164.2890625\n",
      "Epoch: 279, Loss: 23649.474609375\n",
      "Epoch: 280, Loss: 23633.193359375\n",
      "Epoch: 281, Loss: 23650.796875\n",
      "Epoch: 282, Loss: 25140.21484375\n",
      "Epoch: 283, Loss: 25199.435546875\n",
      "Epoch: 284, Loss: 26441.650390625\n",
      "Epoch: 285, Loss: 25333.86328125\n",
      "Epoch: 286, Loss: 24407.072265625\n",
      "Epoch: 287, Loss: 24434.58203125\n",
      "Epoch: 288, Loss: 25258.7578125\n",
      "Epoch: 289, Loss: 24770.40234375\n",
      "Epoch: 290, Loss: 24537.6796875\n",
      "Epoch: 291, Loss: 25487.06640625\n",
      "Epoch: 292, Loss: 24275.904296875\n",
      "Epoch: 293, Loss: 24128.119140625\n",
      "Epoch: 294, Loss: 23497.0234375\n",
      "Epoch: 295, Loss: 24154.017578125\n",
      "Epoch: 296, Loss: 27813.25\n",
      "Epoch: 297, Loss: 28721.578125\n",
      "Epoch: 298, Loss: 26678.423828125\n",
      "Epoch: 299, Loss: 27165.45703125\n",
      "Epoch: 300, Loss: 26337.216796875\n",
      "Epoch: 301, Loss: 26262.43359375\n",
      "Epoch: 302, Loss: 26989.953125\n",
      "Epoch: 303, Loss: 26320.37890625\n",
      "Epoch: 304, Loss: 26203.341796875\n",
      "Epoch: 305, Loss: 26112.10546875\n",
      "Epoch: 306, Loss: 25666.533203125\n",
      "Epoch: 307, Loss: 27171.484375\n",
      "Epoch: 308, Loss: 26580.275390625\n",
      "Epoch: 309, Loss: 25952.30859375\n",
      "Epoch: 310, Loss: 25076.318359375\n",
      "Epoch: 311, Loss: 25075.83984375\n",
      "Epoch: 312, Loss: 24358.3046875\n",
      "Epoch: 313, Loss: 25576.2265625\n",
      "Epoch: 314, Loss: 25053.2265625\n",
      "Epoch: 315, Loss: 24472.41796875\n",
      "Epoch: 316, Loss: 24509.482421875\n",
      "Epoch: 317, Loss: 25187.44921875\n",
      "Epoch: 318, Loss: 24170.060546875\n",
      "Epoch: 319, Loss: 24020.62890625\n",
      "Epoch: 320, Loss: 25276.630859375\n",
      "Epoch: 321, Loss: 25553.412109375\n",
      "Epoch: 322, Loss: 24989.859375\n",
      "Epoch: 323, Loss: 26442.7734375\n",
      "Epoch: 324, Loss: 25367.12109375\n",
      "Epoch: 325, Loss: 25276.244140625\n",
      "Epoch: 326, Loss: 24956.228515625\n",
      "Epoch: 327, Loss: 24713.193359375\n",
      "Epoch: 328, Loss: 25272.087890625\n",
      "Epoch: 329, Loss: 24266.279296875\n",
      "Epoch: 330, Loss: 23482.03515625\n",
      "Epoch: 331, Loss: 25018.08984375\n",
      "Epoch: 332, Loss: 23520.478515625\n",
      "Epoch: 333, Loss: 24844.236328125\n",
      "Epoch: 334, Loss: 27058.125\n",
      "Epoch: 335, Loss: 24588.8359375\n",
      "Epoch: 336, Loss: 26185.029296875\n",
      "Epoch: 337, Loss: 25926.314453125\n",
      "Epoch: 338, Loss: 24687.001953125\n",
      "Epoch: 339, Loss: 25317.494140625\n",
      "Epoch: 340, Loss: 24566.18359375\n",
      "Epoch: 341, Loss: 25426.873046875\n",
      "Epoch: 342, Loss: 23803.740234375\n",
      "Epoch: 343, Loss: 24204.263671875\n",
      "Epoch: 344, Loss: 24026.015625\n",
      "Epoch: 345, Loss: 24058.275390625\n",
      "Epoch: 346, Loss: 24941.97265625\n",
      "Epoch: 347, Loss: 25490.57421875\n",
      "Epoch: 348, Loss: 24761.033203125\n",
      "Epoch: 349, Loss: 24638.380859375\n",
      "Epoch: 350, Loss: 24756.919921875\n",
      "Epoch: 351, Loss: 24366.224609375\n",
      "Epoch: 352, Loss: 24321.0859375\n",
      "Epoch: 353, Loss: 24882.607421875\n",
      "Epoch: 354, Loss: 24738.509765625\n",
      "Epoch: 355, Loss: 24434.97265625\n",
      "Epoch: 356, Loss: 23753.56640625\n",
      "Epoch: 357, Loss: 25946.962890625\n",
      "Epoch: 358, Loss: 22439.34375\n",
      "Epoch: 359, Loss: 23505.4921875\n",
      "Epoch: 360, Loss: 24848.154296875\n",
      "Epoch: 361, Loss: 25537.546875\n",
      "Epoch: 362, Loss: 23602.064453125\n",
      "Epoch: 363, Loss: 26268.482421875\n",
      "Epoch: 364, Loss: 25703.951171875\n",
      "Epoch: 365, Loss: 25603.22265625\n",
      "Epoch: 366, Loss: 24394.005859375\n",
      "Epoch: 367, Loss: 24453.654296875\n",
      "Epoch: 368, Loss: 28634.625\n",
      "Epoch: 369, Loss: 28766.595703125\n",
      "Epoch: 370, Loss: 26769.138671875\n",
      "Epoch: 371, Loss: 27677.876953125\n",
      "Epoch: 372, Loss: 25525.2890625\n",
      "Epoch: 373, Loss: 25898.3125\n",
      "Epoch: 374, Loss: 24880.671875\n",
      "Epoch: 375, Loss: 25282.328125\n",
      "Epoch: 376, Loss: 25484.271484375\n",
      "Epoch: 377, Loss: 25899.62109375\n",
      "Epoch: 378, Loss: 24256.689453125\n",
      "Epoch: 379, Loss: 24115.60546875\n",
      "Epoch: 380, Loss: 24706.17578125\n",
      "Epoch: 381, Loss: 25310.931640625\n",
      "Epoch: 382, Loss: 25313.24609375\n",
      "Epoch: 383, Loss: 25146.412109375\n",
      "Epoch: 384, Loss: 23792.83984375\n",
      "Epoch: 385, Loss: 24874.3203125\n",
      "Epoch: 386, Loss: 23869.416015625\n",
      "Epoch: 387, Loss: 23984.1328125\n",
      "Epoch: 388, Loss: 26985.162109375\n",
      "Epoch: 389, Loss: 25301.5390625\n",
      "Epoch: 390, Loss: 25072.578125\n",
      "Epoch: 391, Loss: 25389.134765625\n",
      "Epoch: 392, Loss: 24839.25390625\n",
      "Epoch: 393, Loss: 24302.220703125\n",
      "Epoch: 394, Loss: 24936.669921875\n",
      "Epoch: 395, Loss: 23825.59765625\n",
      "Epoch: 396, Loss: 26144.986328125\n",
      "Epoch: 397, Loss: 24013.048828125\n",
      "Epoch: 398, Loss: 24839.4453125\n",
      "Epoch: 399, Loss: 26831.25390625\n",
      "Epoch: 400, Loss: 23839.681640625\n",
      "Epoch: 401, Loss: 24693.3359375\n",
      "Epoch: 402, Loss: 24587.802734375\n",
      "Epoch: 403, Loss: 25100.654296875\n",
      "Epoch: 404, Loss: 23614.86328125\n",
      "Epoch: 405, Loss: 23356.025390625\n",
      "Epoch: 406, Loss: 23760.3046875\n",
      "Epoch: 407, Loss: 23538.4140625\n",
      "Epoch: 408, Loss: 26051.884765625\n",
      "Epoch: 409, Loss: 25710.115234375\n",
      "Epoch: 410, Loss: 25645.673828125\n",
      "Epoch: 411, Loss: 24148.453125\n",
      "Epoch: 412, Loss: 26499.232421875\n",
      "Epoch: 413, Loss: 23958.96484375\n",
      "Epoch: 414, Loss: 26276.8515625\n",
      "Epoch: 415, Loss: 24794.955078125\n",
      "Epoch: 416, Loss: 24494.248046875\n",
      "Epoch: 417, Loss: 23819.357421875\n",
      "Epoch: 418, Loss: 24351.740234375\n",
      "Epoch: 419, Loss: 23038.673828125\n",
      "Epoch: 420, Loss: 24010.57421875\n",
      "Epoch: 421, Loss: 24482.943359375\n",
      "Epoch: 422, Loss: 28508.5625\n",
      "Epoch: 423, Loss: 27184.32421875\n",
      "Epoch: 424, Loss: 26251.751953125\n",
      "Epoch: 425, Loss: 26217.134765625\n",
      "Epoch: 426, Loss: 26140.96875\n",
      "Epoch: 427, Loss: 25501.25390625\n",
      "Epoch: 428, Loss: 26282.021484375\n",
      "Epoch: 429, Loss: 24655.189453125\n",
      "Epoch: 430, Loss: 24449.615234375\n",
      "Epoch: 431, Loss: 24119.203125\n",
      "Epoch: 432, Loss: 25207.578125\n",
      "Epoch: 433, Loss: 24575.7109375\n",
      "Epoch: 434, Loss: 24236.115234375\n",
      "Epoch: 435, Loss: 25311.71875\n",
      "Epoch: 436, Loss: 25418.41796875\n",
      "Epoch: 437, Loss: 25205.55078125\n",
      "Epoch: 438, Loss: 24571.29296875\n",
      "Epoch: 439, Loss: 23754.40625\n",
      "Epoch: 440, Loss: 23775.259765625\n",
      "Epoch: 441, Loss: 24196.833984375\n",
      "Epoch: 442, Loss: 24646.8125\n",
      "Epoch: 443, Loss: 23720.455078125\n",
      "Epoch: 444, Loss: 25931.900390625\n",
      "Epoch: 445, Loss: 24142.25390625\n",
      "Epoch: 446, Loss: 25178.921875\n",
      "Epoch: 447, Loss: 22824.587890625\n",
      "Epoch: 448, Loss: 24278.39453125\n",
      "Epoch: 449, Loss: 23630.640625\n",
      "Epoch: 450, Loss: 23816.2734375\n",
      "Epoch: 451, Loss: 24032.45703125\n",
      "Epoch: 452, Loss: 23939.8828125\n",
      "Epoch: 453, Loss: 24636.51171875\n",
      "Epoch: 454, Loss: 26329.044921875\n",
      "Epoch: 455, Loss: 24166.3125\n",
      "Epoch: 456, Loss: 25229.169921875\n",
      "Epoch: 457, Loss: 23971.482421875\n",
      "Epoch: 458, Loss: 24604.32421875\n",
      "Epoch: 459, Loss: 23832.498046875\n",
      "Epoch: 460, Loss: 23485.486328125\n",
      "Epoch: 461, Loss: 22756.76171875\n",
      "Epoch: 462, Loss: 23394.12109375\n",
      "Epoch: 463, Loss: 23604.98046875\n",
      "Epoch: 464, Loss: 23673.732421875\n",
      "Epoch: 465, Loss: 24846.75390625\n",
      "Epoch: 466, Loss: 24443.630859375\n",
      "Epoch: 467, Loss: 23899.486328125\n",
      "Epoch: 468, Loss: 24766.458984375\n",
      "Epoch: 469, Loss: 23354.87109375\n",
      "Epoch: 470, Loss: 24334.505859375\n",
      "Epoch: 471, Loss: 22254.208984375\n",
      "Epoch: 472, Loss: 22858.39453125\n",
      "Epoch: 473, Loss: 24065.810546875\n",
      "Epoch: 474, Loss: 24043.560546875\n",
      "Epoch: 475, Loss: 26152.30078125\n",
      "Epoch: 476, Loss: 24604.2109375\n",
      "Epoch: 477, Loss: 24588.732421875\n",
      "Epoch: 478, Loss: 24908.693359375\n",
      "Epoch: 479, Loss: 23972.53125\n",
      "Epoch: 480, Loss: 24988.990234375\n",
      "Epoch: 481, Loss: 24321.6796875\n",
      "Epoch: 482, Loss: 24094.8515625\n",
      "Epoch: 483, Loss: 24663.26953125\n",
      "Epoch: 484, Loss: 24246.822265625\n",
      "Epoch: 485, Loss: 24614.96875\n",
      "Epoch: 486, Loss: 23878.46484375\n",
      "Epoch: 487, Loss: 23823.765625\n",
      "Epoch: 488, Loss: 23261.79296875\n",
      "Epoch: 489, Loss: 24111.900390625\n",
      "Epoch: 490, Loss: 23904.15625\n",
      "Epoch: 491, Loss: 23522.96484375\n",
      "Epoch: 492, Loss: 24710.052734375\n",
      "Epoch: 493, Loss: 24489.359375\n",
      "Epoch: 494, Loss: 24871.33203125\n",
      "Epoch: 495, Loss: 23445.275390625\n",
      "Epoch: 496, Loss: 26137.076171875\n",
      "Epoch: 497, Loss: 26146.017578125\n",
      "Epoch: 498, Loss: 25191.208984375\n",
      "Epoch: 499, Loss: 23581.46875\n"
     ]
    }
   ],
   "source": [
    "model.reset()\n",
    "model.learn(\n",
    "    x_train_df=train_df.drop(columns=[\"SalePrice\"], inplace=False),\n",
    "    y_train_df=pd.DataFrame(train_df[\"SalePrice\"]),\n",
    "    epochs=500, learning_rate=0.5, weight_decay=0.1\n",
    ")\n",
    "predictions = model.predict(test_df).detach().numpy()\n",
    "make_kaggle_submission_file(predictions, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.004808\n",
      "0:\tlearn: 79169.9295586\ttotal: 23.3ms\tremaining: 5m 48s\n",
      "250:\tlearn: 43667.4253451\ttotal: 1.72s\tremaining: 1m 40s\n",
      "500:\tlearn: 30874.6197873\ttotal: 3.24s\tremaining: 1m 33s\n",
      "750:\tlearn: 25404.2889562\ttotal: 4.63s\tremaining: 1m 27s\n",
      "1000:\tlearn: 22629.8463694\ttotal: 6.08s\tremaining: 1m 25s\n",
      "1250:\tlearn: 20915.7963649\ttotal: 7.47s\tremaining: 1m 22s\n",
      "1500:\tlearn: 19663.4328627\ttotal: 8.73s\tremaining: 1m 18s\n",
      "1750:\tlearn: 18631.7390518\ttotal: 10.3s\tremaining: 1m 17s\n",
      "2000:\tlearn: 17709.1985461\ttotal: 11.7s\tremaining: 1m 15s\n",
      "2250:\tlearn: 16941.6500068\ttotal: 13.1s\tremaining: 1m 14s\n",
      "2500:\tlearn: 16294.4989642\ttotal: 14.5s\tremaining: 1m 12s\n",
      "2750:\tlearn: 15714.6912783\ttotal: 15.9s\tremaining: 1m 10s\n",
      "3000:\tlearn: 15222.0871982\ttotal: 17.3s\tremaining: 1m 9s\n",
      "3250:\tlearn: 14794.4746812\ttotal: 18.7s\tremaining: 1m 7s\n",
      "3500:\tlearn: 14398.6278099\ttotal: 20.1s\tremaining: 1m 5s\n",
      "3750:\tlearn: 14042.9421487\ttotal: 21.4s\tremaining: 1m 4s\n",
      "4000:\tlearn: 13682.3142411\ttotal: 22.7s\tremaining: 1m 2s\n",
      "4250:\tlearn: 13375.2283311\ttotal: 24s\tremaining: 1m\n",
      "4500:\tlearn: 13080.0568648\ttotal: 25.3s\tremaining: 59s\n",
      "4750:\tlearn: 12803.8682358\ttotal: 26.6s\tremaining: 57.4s\n",
      "5000:\tlearn: 12539.5450974\ttotal: 27.9s\tremaining: 55.8s\n",
      "5250:\tlearn: 12269.3852745\ttotal: 29s\tremaining: 53.8s\n",
      "5500:\tlearn: 12026.7950905\ttotal: 30.2s\tremaining: 52.2s\n",
      "5750:\tlearn: 11783.1973684\ttotal: 31.7s\tremaining: 51s\n",
      "6000:\tlearn: 11553.0887819\ttotal: 33.2s\tremaining: 49.8s\n",
      "6250:\tlearn: 11333.0020981\ttotal: 34.4s\tremaining: 48.2s\n",
      "6500:\tlearn: 11129.0080050\ttotal: 35.7s\tremaining: 46.7s\n",
      "6750:\tlearn: 10942.8228532\ttotal: 37.2s\tremaining: 45.5s\n",
      "7000:\tlearn: 10752.8755209\ttotal: 38.7s\tremaining: 44.2s\n",
      "7250:\tlearn: 10566.2703407\ttotal: 40.3s\tremaining: 43s\n",
      "7500:\tlearn: 10372.3971072\ttotal: 41.4s\tremaining: 41.4s\n",
      "7750:\tlearn: 10202.1479597\ttotal: 42.4s\tremaining: 39.7s\n",
      "8000:\tlearn: 10035.4855652\ttotal: 43.8s\tremaining: 38.3s\n",
      "8250:\tlearn: 9858.4988865\ttotal: 45.1s\tremaining: 36.9s\n",
      "8500:\tlearn: 9680.5364334\ttotal: 46.3s\tremaining: 35.4s\n",
      "8750:\tlearn: 9513.5193413\ttotal: 47.5s\tremaining: 33.9s\n",
      "9000:\tlearn: 9356.2625856\ttotal: 49.1s\tremaining: 32.7s\n",
      "9250:\tlearn: 9212.2153153\ttotal: 50.6s\tremaining: 31.5s\n",
      "9500:\tlearn: 9067.4339108\ttotal: 52.1s\tremaining: 30.2s\n",
      "9750:\tlearn: 8926.6662754\ttotal: 53.4s\tremaining: 28.7s\n",
      "10000:\tlearn: 8790.0195475\ttotal: 54.7s\tremaining: 27.4s\n",
      "10250:\tlearn: 8651.3468648\ttotal: 56.1s\tremaining: 26s\n",
      "10500:\tlearn: 8519.0159954\ttotal: 57.6s\tremaining: 24.7s\n",
      "10750:\tlearn: 8402.0092269\ttotal: 58.9s\tremaining: 23.3s\n",
      "11000:\tlearn: 8275.0490212\ttotal: 1m\tremaining: 21.9s\n",
      "11250:\tlearn: 8148.1729075\ttotal: 1m 1s\tremaining: 20.6s\n",
      "11500:\tlearn: 8020.8683813\ttotal: 1m 2s\tremaining: 19.1s\n",
      "11750:\tlearn: 7895.0847804\ttotal: 1m 4s\tremaining: 17.8s\n",
      "12000:\tlearn: 7776.7816553\ttotal: 1m 5s\tremaining: 16.4s\n",
      "12250:\tlearn: 7658.7398422\ttotal: 1m 7s\tremaining: 15.1s\n",
      "12500:\tlearn: 7547.6056021\ttotal: 1m 8s\tremaining: 13.7s\n",
      "12750:\tlearn: 7440.9778758\ttotal: 1m 9s\tremaining: 12.3s\n",
      "13000:\tlearn: 7342.0697529\ttotal: 1m 11s\tremaining: 11s\n",
      "13250:\tlearn: 7236.5786347\ttotal: 1m 12s\tremaining: 9.61s\n",
      "13500:\tlearn: 7131.1068368\ttotal: 1m 14s\tremaining: 8.28s\n",
      "13750:\tlearn: 7033.3208521\ttotal: 1m 16s\tremaining: 6.92s\n",
      "14000:\tlearn: 6942.7784610\ttotal: 1m 17s\tremaining: 5.55s\n",
      "14250:\tlearn: 6842.3289548\ttotal: 1m 19s\tremaining: 4.17s\n",
      "14500:\tlearn: 6754.3666319\ttotal: 1m 21s\tremaining: 2.8s\n",
      "14750:\tlearn: 6657.8174549\ttotal: 1m 22s\tremaining: 1.39s\n",
      "14999:\tlearn: 6573.2426030\ttotal: 1m 24s\tremaining: 0us\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m catboost \u001b[38;5;241m=\u001b[39m CatBoost(early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15000\u001b[39m, rsm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      3\u001b[0m catboost\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m      4\u001b[0m     x_train_df\u001b[38;5;241m=\u001b[39mtrain_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     y_train_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m predictions_catboost \u001b[38;5;241m=\u001b[39m \u001b[43mcatboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m nn \u001b[38;5;241m=\u001b[39m NN(hidden_size1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1028\u001b[39m, hidden_size2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, hidden_size3\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      9\u001b[0m nn\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m     10\u001b[0m     x_train_df\u001b[38;5;241m=\u001b[39mtrain_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     11\u001b[0m     y_train_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     12\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "# ensemble of catboost and nn\n",
    "catboost = CatBoost(early_stopping_rounds=2000, iterations=15000, rsm=0.1)\n",
    "catboost.learn(\n",
    "    x_train_df=train_df.drop(columns=[\"SalePrice\"], inplace=False),\n",
    "    y_train_df=pd.DataFrame(train_df[\"SalePrice\"])\n",
    ")\n",
    "predictions_catboost = catboost.predict(test_df)\n",
    "nn = NN(hidden_size1=1028, hidden_size2=512, hidden_size3=256)\n",
    "nn.learn(\n",
    "    x_train_df=train_df.drop(columns=[\"SalePrice\"], inplace=False),\n",
    "    y_train_df=pd.DataFrame(train_df[\"SalePrice\"]),\n",
    "    epochs=500, learning_rate=0.5, weight_decay=0.1\n",
    ")\n",
    "predictions_nn = nn.predict(test_df).detach().numpy()\n",
    "predictions = (predictions_catboost + predictions_nn) / 2\n",
    "make_kaggle_submission_file(predictions, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111020/546932487.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  predictions = [float(predictions_catboost[i] + predictions_nn[i])/2 for i in range(len(predictions_catboost))]\n"
     ]
    }
   ],
   "source": [
    "# sum to arrays of length i into an array of length i\n",
    "predictions = [float(predictions_catboost[i] + predictions_nn[i])/2 for i in range(len(predictions_catboost))]\n",
    "make_kaggle_submission_file(predictions_nn, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 197583.890625\n",
      "Epoch: 1, Loss: 197553.875\n",
      "Epoch: 2, Loss: 197490.203125\n",
      "Epoch: 3, Loss: 197366.1875\n",
      "Epoch: 4, Loss: 197149.265625\n",
      "Epoch: 5, Loss: 196810.78125\n",
      "Epoch: 6, Loss: 196338.859375\n",
      "Epoch: 7, Loss: 195850.859375\n",
      "Epoch: 8, Loss: 195306.625\n",
      "Epoch: 9, Loss: 194110.6875\n",
      "Epoch: 10, Loss: 193028.25\n",
      "Epoch: 11, Loss: 191781.40625\n",
      "Epoch: 12, Loss: 190373.140625\n",
      "Epoch: 13, Loss: 188809.625\n",
      "Epoch: 14, Loss: 187000.734375\n",
      "Epoch: 15, Loss: 184946.015625\n",
      "Epoch: 16, Loss: 182643.671875\n",
      "Epoch: 17, Loss: 180202.328125\n",
      "Epoch: 18, Loss: 177622.90625\n",
      "Epoch: 19, Loss: 174428.421875\n",
      "Epoch: 20, Loss: 171282.1875\n",
      "Epoch: 21, Loss: 168285.453125\n",
      "Epoch: 22, Loss: 164239.921875\n",
      "Epoch: 23, Loss: 160203.859375\n",
      "Epoch: 24, Loss: 156082.6875\n",
      "Epoch: 25, Loss: 151425.0\n",
      "Epoch: 26, Loss: 146608.609375\n",
      "Epoch: 27, Loss: 142081.109375\n",
      "Epoch: 28, Loss: 136591.046875\n",
      "Epoch: 29, Loss: 131014.1484375\n",
      "Epoch: 30, Loss: 125506.7265625\n",
      "Epoch: 31, Loss: 120684.796875\n",
      "Epoch: 32, Loss: 113829.2421875\n",
      "Epoch: 33, Loss: 106888.5234375\n",
      "Epoch: 34, Loss: 99533.3828125\n",
      "Epoch: 35, Loss: 93549.5546875\n",
      "Epoch: 36, Loss: 86957.3515625\n",
      "Epoch: 37, Loss: 79991.78125\n",
      "Epoch: 38, Loss: 74258.65625\n",
      "Epoch: 39, Loss: 79304.9453125\n",
      "Epoch: 40, Loss: 63149.5546875\n",
      "Epoch: 41, Loss: 60526.47265625\n",
      "Epoch: 42, Loss: 52340.01953125\n",
      "Epoch: 43, Loss: 50935.34765625\n",
      "Epoch: 44, Loss: 46680.53515625\n",
      "Epoch: 45, Loss: 45155.23046875\n",
      "Epoch: 46, Loss: 45272.93359375\n",
      "Epoch: 47, Loss: 46574.59375\n",
      "Epoch: 48, Loss: 45690.00390625\n",
      "Epoch: 49, Loss: 45581.6875\n",
      "Epoch: 50, Loss: 45054.19921875\n",
      "Epoch: 51, Loss: 43578.0859375\n",
      "Epoch: 52, Loss: 41769.64453125\n",
      "Epoch: 53, Loss: 36533.6953125\n",
      "Epoch: 54, Loss: 32583.80859375\n",
      "Epoch: 55, Loss: 37495.0\n",
      "Epoch: 56, Loss: 45291.68359375\n",
      "Epoch: 57, Loss: 35014.55078125\n",
      "Epoch: 58, Loss: 35491.765625\n",
      "Epoch: 59, Loss: 35002.61328125\n",
      "Epoch: 60, Loss: 34595.87109375\n",
      "Epoch: 61, Loss: 33872.31640625\n",
      "Epoch: 62, Loss: 33379.00390625\n",
      "Epoch: 63, Loss: 33421.3125\n",
      "Epoch: 64, Loss: 31896.8671875\n",
      "Epoch: 65, Loss: 32413.626953125\n",
      "Epoch: 66, Loss: 29990.3203125\n",
      "Epoch: 67, Loss: 30274.369140625\n",
      "Epoch: 68, Loss: 31074.7734375\n",
      "Epoch: 69, Loss: 33201.0859375\n",
      "Epoch: 70, Loss: 29345.41015625\n",
      "Epoch: 71, Loss: 30958.158203125\n",
      "Epoch: 72, Loss: 30934.533203125\n",
      "Epoch: 73, Loss: 33229.6640625\n",
      "Epoch: 74, Loss: 33206.9921875\n",
      "Epoch: 75, Loss: 31114.306640625\n",
      "Epoch: 76, Loss: 30481.537109375\n",
      "Epoch: 77, Loss: 30421.951171875\n",
      "Epoch: 78, Loss: 31767.1171875\n",
      "Epoch: 79, Loss: 29749.078125\n",
      "Epoch: 80, Loss: 30668.28125\n",
      "Epoch: 81, Loss: 29644.712890625\n",
      "Epoch: 82, Loss: 29522.7109375\n",
      "Epoch: 83, Loss: 28590.623046875\n",
      "Epoch: 84, Loss: 29554.271484375\n",
      "Epoch: 85, Loss: 27880.01953125\n",
      "Epoch: 86, Loss: 27786.599609375\n",
      "Epoch: 87, Loss: 27997.306640625\n",
      "Epoch: 88, Loss: 27539.296875\n",
      "Epoch: 89, Loss: 27437.591796875\n",
      "Epoch: 90, Loss: 26948.162109375\n",
      "Epoch: 91, Loss: 26931.6640625\n",
      "Epoch: 92, Loss: 27317.224609375\n",
      "Epoch: 93, Loss: 26633.20703125\n",
      "Epoch: 94, Loss: 26780.8203125\n",
      "Epoch: 95, Loss: 27563.84765625\n",
      "Epoch: 96, Loss: 27501.3671875\n",
      "Epoch: 97, Loss: 26758.134765625\n",
      "Epoch: 98, Loss: 28329.541015625\n",
      "Epoch: 99, Loss: 28708.64453125\n",
      "Epoch: 100, Loss: 27657.931640625\n",
      "Epoch: 101, Loss: 26949.16015625\n",
      "Epoch: 102, Loss: 26368.177734375\n",
      "Epoch: 103, Loss: 29382.47265625\n",
      "Epoch: 104, Loss: 29277.32421875\n",
      "Epoch: 105, Loss: 30649.451171875\n",
      "Epoch: 106, Loss: 29398.8671875\n",
      "Epoch: 107, Loss: 28860.77734375\n",
      "Epoch: 108, Loss: 28489.130859375\n",
      "Epoch: 109, Loss: 28511.54296875\n",
      "Epoch: 110, Loss: 27728.970703125\n",
      "Epoch: 111, Loss: 26480.01171875\n",
      "Epoch: 112, Loss: 26366.681640625\n",
      "Epoch: 113, Loss: 26713.0859375\n",
      "Epoch: 114, Loss: 26597.580078125\n",
      "Epoch: 115, Loss: 26187.28515625\n",
      "Epoch: 116, Loss: 26406.685546875\n",
      "Epoch: 117, Loss: 26785.767578125\n",
      "Epoch: 118, Loss: 25970.03125\n",
      "Epoch: 119, Loss: 25683.017578125\n",
      "Epoch: 120, Loss: 27566.736328125\n",
      "Epoch: 121, Loss: 25071.1328125\n",
      "Epoch: 122, Loss: 27074.69921875\n",
      "Epoch: 123, Loss: 25667.021484375\n",
      "Epoch: 124, Loss: 26803.720703125\n",
      "Epoch: 125, Loss: 27021.5859375\n",
      "Epoch: 126, Loss: 27822.197265625\n",
      "Epoch: 127, Loss: 27769.8046875\n",
      "Epoch: 128, Loss: 28098.5703125\n",
      "Epoch: 129, Loss: 27486.34375\n",
      "Epoch: 130, Loss: 27371.484375\n",
      "Epoch: 131, Loss: 27267.3046875\n",
      "Epoch: 132, Loss: 26092.1953125\n",
      "Epoch: 133, Loss: 25879.896484375\n",
      "Epoch: 134, Loss: 25929.05078125\n",
      "Epoch: 135, Loss: 26624.431640625\n",
      "Epoch: 136, Loss: 25148.041015625\n",
      "Epoch: 137, Loss: 25761.75390625\n",
      "Epoch: 138, Loss: 25012.509765625\n",
      "Epoch: 139, Loss: 25700.41015625\n",
      "Epoch: 140, Loss: 26653.478515625\n",
      "Epoch: 141, Loss: 28187.845703125\n",
      "Epoch: 142, Loss: 29149.134765625\n",
      "Epoch: 143, Loss: 27348.595703125\n",
      "Epoch: 144, Loss: 27007.2421875\n",
      "Epoch: 145, Loss: 27885.556640625\n",
      "Epoch: 146, Loss: 28253.09765625\n",
      "Epoch: 147, Loss: 28643.857421875\n",
      "Epoch: 148, Loss: 27279.5703125\n",
      "Epoch: 149, Loss: 27771.283203125\n",
      "Epoch: 150, Loss: 26401.837890625\n",
      "Epoch: 151, Loss: 25931.912109375\n",
      "Epoch: 152, Loss: 26557.919921875\n",
      "Epoch: 153, Loss: 26414.0859375\n",
      "Epoch: 154, Loss: 25111.697265625\n",
      "Epoch: 155, Loss: 25498.90234375\n",
      "Epoch: 156, Loss: 26811.2578125\n",
      "Epoch: 157, Loss: 25500.21875\n",
      "Epoch: 158, Loss: 26283.81640625\n",
      "Epoch: 159, Loss: 25748.3515625\n",
      "Epoch: 160, Loss: 24731.724609375\n",
      "Epoch: 161, Loss: 26056.083984375\n",
      "Epoch: 162, Loss: 25910.630859375\n",
      "Epoch: 163, Loss: 26996.7734375\n",
      "Epoch: 164, Loss: 25460.501953125\n",
      "Epoch: 165, Loss: 25120.994140625\n",
      "Epoch: 166, Loss: 24907.01953125\n",
      "Epoch: 167, Loss: 24777.908203125\n",
      "Epoch: 168, Loss: 25268.2109375\n",
      "Epoch: 169, Loss: 25744.416015625\n",
      "Epoch: 170, Loss: 26193.953125\n",
      "Epoch: 171, Loss: 25762.2265625\n",
      "Epoch: 172, Loss: 24880.169921875\n",
      "Epoch: 173, Loss: 26143.974609375\n",
      "Epoch: 174, Loss: 25245.888671875\n",
      "Epoch: 175, Loss: 24502.65625\n",
      "Epoch: 176, Loss: 25533.0703125\n",
      "Epoch: 177, Loss: 25164.4375\n",
      "Epoch: 178, Loss: 24824.19921875\n",
      "Epoch: 179, Loss: 24587.24609375\n",
      "Epoch: 180, Loss: 24303.404296875\n",
      "Epoch: 181, Loss: 24634.26953125\n",
      "Epoch: 182, Loss: 25524.486328125\n",
      "Epoch: 183, Loss: 24001.716796875\n",
      "Epoch: 184, Loss: 26171.552734375\n",
      "Epoch: 185, Loss: 26398.78125\n",
      "Epoch: 186, Loss: 25649.671875\n",
      "Epoch: 187, Loss: 25966.2109375\n",
      "Epoch: 188, Loss: 26515.599609375\n",
      "Epoch: 189, Loss: 28807.09375\n",
      "Epoch: 190, Loss: 27160.751953125\n",
      "Epoch: 191, Loss: 26703.60546875\n",
      "Epoch: 192, Loss: 26573.728515625\n",
      "Epoch: 193, Loss: 27681.34765625\n",
      "Epoch: 194, Loss: 26912.767578125\n",
      "Epoch: 195, Loss: 26430.59375\n",
      "Epoch: 196, Loss: 25789.662109375\n",
      "Epoch: 197, Loss: 26479.8515625\n",
      "Epoch: 198, Loss: 26723.80859375\n",
      "Epoch: 199, Loss: 25906.041015625\n",
      "Epoch: 200, Loss: 26735.255859375\n",
      "Epoch: 201, Loss: 24968.552734375\n",
      "Epoch: 202, Loss: 25044.037109375\n",
      "Epoch: 203, Loss: 24694.83984375\n",
      "Epoch: 204, Loss: 25968.5859375\n",
      "Epoch: 205, Loss: 24779.619140625\n",
      "Epoch: 206, Loss: 23957.658203125\n",
      "Epoch: 207, Loss: 25293.654296875\n",
      "Epoch: 208, Loss: 24394.1328125\n",
      "Epoch: 209, Loss: 24011.50390625\n",
      "Epoch: 210, Loss: 25311.76171875\n",
      "Epoch: 211, Loss: 24040.7109375\n",
      "Epoch: 212, Loss: 24090.6484375\n",
      "Epoch: 213, Loss: 24618.615234375\n",
      "Epoch: 214, Loss: 24197.90234375\n",
      "Epoch: 215, Loss: 25258.783203125\n",
      "Epoch: 216, Loss: 24840.712890625\n",
      "Epoch: 217, Loss: 23740.248046875\n",
      "Epoch: 218, Loss: 25046.708984375\n",
      "Epoch: 219, Loss: 23145.94921875\n",
      "Epoch: 220, Loss: 24223.037109375\n",
      "Epoch: 221, Loss: 23765.263671875\n",
      "Epoch: 222, Loss: 24662.412109375\n",
      "Epoch: 223, Loss: 23975.224609375\n",
      "Epoch: 224, Loss: 23360.908203125\n",
      "Epoch: 225, Loss: 23667.71875\n",
      "Epoch: 226, Loss: 24553.107421875\n",
      "Epoch: 227, Loss: 24978.25390625\n",
      "Epoch: 228, Loss: 25019.21875\n",
      "Epoch: 229, Loss: 26221.12890625\n",
      "Epoch: 230, Loss: 24870.759765625\n",
      "Epoch: 231, Loss: 25600.427734375\n",
      "Epoch: 232, Loss: 24508.130859375\n",
      "Epoch: 233, Loss: 24518.05859375\n",
      "Epoch: 234, Loss: 24990.337890625\n",
      "Epoch: 235, Loss: 24603.3125\n",
      "Epoch: 236, Loss: 24819.7578125\n",
      "Epoch: 237, Loss: 25154.427734375\n",
      "Epoch: 238, Loss: 25151.09765625\n",
      "Epoch: 239, Loss: 26108.3125\n",
      "Epoch: 240, Loss: 26434.744140625\n",
      "Epoch: 241, Loss: 25096.431640625\n",
      "Epoch: 242, Loss: 25915.42578125\n",
      "Epoch: 243, Loss: 25743.1875\n",
      "Epoch: 244, Loss: 26658.787109375\n",
      "Epoch: 245, Loss: 25176.333984375\n",
      "Epoch: 246, Loss: 25780.099609375\n",
      "Epoch: 247, Loss: 24643.490234375\n",
      "Epoch: 248, Loss: 24125.75390625\n",
      "Epoch: 249, Loss: 24859.03515625\n",
      "Epoch: 250, Loss: 23783.27734375\n",
      "Epoch: 251, Loss: 25349.375\n",
      "Epoch: 252, Loss: 25096.115234375\n",
      "Epoch: 253, Loss: 24495.33984375\n",
      "Epoch: 254, Loss: 24106.70703125\n",
      "Epoch: 255, Loss: 25334.8046875\n",
      "Epoch: 256, Loss: 26495.630859375\n",
      "Epoch: 257, Loss: 26974.982421875\n",
      "Epoch: 258, Loss: 26519.955078125\n",
      "Epoch: 259, Loss: 26785.8046875\n",
      "Epoch: 260, Loss: 25487.640625\n",
      "Epoch: 261, Loss: 25890.091796875\n",
      "Epoch: 262, Loss: 25649.03125\n",
      "Epoch: 263, Loss: 26453.919921875\n",
      "Epoch: 264, Loss: 25450.0390625\n",
      "Epoch: 265, Loss: 24759.9140625\n",
      "Epoch: 266, Loss: 25121.73046875\n",
      "Epoch: 267, Loss: 24750.58984375\n",
      "Epoch: 268, Loss: 24879.890625\n",
      "Epoch: 269, Loss: 24945.28125\n",
      "Epoch: 270, Loss: 24101.986328125\n",
      "Epoch: 271, Loss: 25650.439453125\n",
      "Epoch: 272, Loss: 25931.43359375\n",
      "Epoch: 273, Loss: 25247.173828125\n",
      "Epoch: 274, Loss: 24880.681640625\n",
      "Epoch: 275, Loss: 24810.970703125\n",
      "Epoch: 276, Loss: 23735.50390625\n",
      "Epoch: 277, Loss: 25712.40625\n",
      "Epoch: 278, Loss: 24008.013671875\n",
      "Epoch: 279, Loss: 25627.869140625\n",
      "Epoch: 280, Loss: 24989.013671875\n",
      "Epoch: 281, Loss: 24111.06640625\n",
      "Epoch: 282, Loss: 25527.142578125\n",
      "Epoch: 283, Loss: 23707.0625\n",
      "Epoch: 284, Loss: 24346.451171875\n",
      "Epoch: 285, Loss: 23789.931640625\n",
      "Epoch: 286, Loss: 24863.59375\n",
      "Epoch: 287, Loss: 24536.3359375\n",
      "Epoch: 288, Loss: 23209.736328125\n",
      "Epoch: 289, Loss: 24639.97265625\n",
      "Epoch: 290, Loss: 24085.455078125\n",
      "Epoch: 291, Loss: 23731.513671875\n",
      "Epoch: 292, Loss: 24732.013671875\n",
      "Epoch: 293, Loss: 23605.07421875\n",
      "Epoch: 294, Loss: 24955.001953125\n",
      "Epoch: 295, Loss: 23374.62109375\n",
      "Epoch: 296, Loss: 23011.994140625\n",
      "Epoch: 297, Loss: 25021.111328125\n",
      "Epoch: 298, Loss: 24930.09765625\n",
      "Epoch: 299, Loss: 25243.955078125\n",
      "Epoch: 300, Loss: 26037.669921875\n",
      "Epoch: 301, Loss: 26011.509765625\n",
      "Epoch: 302, Loss: 25215.31640625\n",
      "Epoch: 303, Loss: 24242.890625\n",
      "Epoch: 304, Loss: 26471.80078125\n",
      "Epoch: 305, Loss: 24172.486328125\n",
      "Epoch: 306, Loss: 24736.7421875\n",
      "Epoch: 307, Loss: 24282.65234375\n",
      "Epoch: 308, Loss: 24421.201171875\n",
      "Epoch: 309, Loss: 23051.794921875\n",
      "Epoch: 310, Loss: 25366.322265625\n",
      "Epoch: 311, Loss: 24773.92578125\n",
      "Epoch: 312, Loss: 23516.75390625\n",
      "Epoch: 313, Loss: 23773.65625\n",
      "Epoch: 314, Loss: 23327.712890625\n",
      "Epoch: 315, Loss: 23950.978515625\n",
      "Epoch: 316, Loss: 23965.35546875\n",
      "Epoch: 317, Loss: 24506.18359375\n",
      "Epoch: 318, Loss: 23745.650390625\n",
      "Epoch: 319, Loss: 23440.205078125\n",
      "Epoch: 320, Loss: 23841.67578125\n",
      "Epoch: 321, Loss: 26438.84375\n",
      "Epoch: 322, Loss: 24064.2265625\n",
      "Epoch: 323, Loss: 24495.37109375\n",
      "Epoch: 324, Loss: 24684.205078125\n",
      "Epoch: 325, Loss: 25263.978515625\n",
      "Epoch: 326, Loss: 24164.041015625\n",
      "Epoch: 327, Loss: 24251.744140625\n",
      "Epoch: 328, Loss: 24736.89453125\n",
      "Epoch: 329, Loss: 23977.84375\n",
      "Epoch: 330, Loss: 24102.3984375\n",
      "Epoch: 331, Loss: 25210.287109375\n",
      "Epoch: 332, Loss: 26981.634765625\n",
      "Epoch: 333, Loss: 28543.462890625\n",
      "Epoch: 334, Loss: 28686.64453125\n",
      "Epoch: 335, Loss: 27589.419921875\n",
      "Epoch: 336, Loss: 29337.498046875\n",
      "Epoch: 337, Loss: 27365.6875\n",
      "Epoch: 338, Loss: 29064.900390625\n",
      "Epoch: 339, Loss: 26186.93359375\n",
      "Epoch: 340, Loss: 26197.384765625\n",
      "Epoch: 341, Loss: 25720.84765625\n",
      "Epoch: 342, Loss: 26449.982421875\n",
      "Epoch: 343, Loss: 25546.81640625\n",
      "Epoch: 344, Loss: 25492.34375\n",
      "Epoch: 345, Loss: 24225.7890625\n",
      "Epoch: 346, Loss: 25058.43359375\n",
      "Epoch: 347, Loss: 25896.451171875\n",
      "Epoch: 348, Loss: 27292.220703125\n",
      "Epoch: 349, Loss: 24999.142578125\n",
      "Epoch: 350, Loss: 24052.759765625\n",
      "Epoch: 351, Loss: 27255.845703125\n",
      "Epoch: 352, Loss: 26206.119140625\n",
      "Epoch: 353, Loss: 26800.44921875\n",
      "Epoch: 354, Loss: 25491.044921875\n",
      "Epoch: 355, Loss: 24488.080078125\n",
      "Epoch: 356, Loss: 26794.033203125\n",
      "Epoch: 357, Loss: 27491.009765625\n",
      "Epoch: 358, Loss: 25500.908203125\n",
      "Epoch: 359, Loss: 25425.931640625\n",
      "Epoch: 360, Loss: 27294.001953125\n",
      "Epoch: 361, Loss: 26559.314453125\n",
      "Epoch: 362, Loss: 25648.57421875\n",
      "Epoch: 363, Loss: 25513.876953125\n",
      "Epoch: 364, Loss: 25243.994140625\n",
      "Epoch: 365, Loss: 25234.8203125\n",
      "Epoch: 366, Loss: 24251.439453125\n",
      "Epoch: 367, Loss: 24463.2421875\n",
      "Epoch: 368, Loss: 33964.50390625\n",
      "Epoch: 369, Loss: 33021.19921875\n",
      "Epoch: 370, Loss: 31066.84765625\n",
      "Epoch: 371, Loss: 30606.744140625\n",
      "Epoch: 372, Loss: 32200.525390625\n",
      "Epoch: 373, Loss: 31380.560546875\n",
      "Epoch: 374, Loss: 29695.435546875\n",
      "Epoch: 375, Loss: 30353.16015625\n",
      "Epoch: 376, Loss: 29015.0078125\n",
      "Epoch: 377, Loss: 28620.638671875\n",
      "Epoch: 378, Loss: 27673.775390625\n",
      "Epoch: 379, Loss: 28619.634765625\n",
      "Epoch: 380, Loss: 28120.435546875\n",
      "Epoch: 381, Loss: 27388.939453125\n",
      "Epoch: 382, Loss: 27348.814453125\n",
      "Epoch: 383, Loss: 26413.7265625\n",
      "Epoch: 384, Loss: 26729.876953125\n",
      "Epoch: 385, Loss: 25487.53125\n",
      "Epoch: 386, Loss: 26270.12890625\n",
      "Epoch: 387, Loss: 26559.130859375\n",
      "Epoch: 388, Loss: 25689.693359375\n",
      "Epoch: 389, Loss: 26247.15625\n",
      "Epoch: 390, Loss: 25799.40234375\n",
      "Epoch: 391, Loss: 25568.779296875\n",
      "Epoch: 392, Loss: 24776.501953125\n",
      "Epoch: 393, Loss: 26107.39453125\n",
      "Epoch: 394, Loss: 25758.919921875\n",
      "Epoch: 395, Loss: 26246.087890625\n",
      "Epoch: 396, Loss: 26886.330078125\n",
      "Epoch: 397, Loss: 24723.033203125\n",
      "Epoch: 398, Loss: 26121.25390625\n",
      "Epoch: 399, Loss: 24768.1953125\n",
      "Epoch: 400, Loss: 24299.779296875\n",
      "Epoch: 401, Loss: 24704.25390625\n",
      "Epoch: 402, Loss: 24407.4765625\n",
      "Epoch: 403, Loss: 28535.638671875\n",
      "Epoch: 404, Loss: 27218.748046875\n",
      "Epoch: 405, Loss: 26791.939453125\n",
      "Epoch: 406, Loss: 26399.12109375\n",
      "Epoch: 407, Loss: 26228.61328125\n",
      "Epoch: 408, Loss: 26153.625\n",
      "Epoch: 409, Loss: 24775.353515625\n",
      "Epoch: 410, Loss: 24788.431640625\n",
      "Epoch: 411, Loss: 26047.193359375\n",
      "Epoch: 412, Loss: 24509.888671875\n",
      "Epoch: 413, Loss: 25597.025390625\n",
      "Epoch: 414, Loss: 24680.017578125\n",
      "Epoch: 415, Loss: 24477.265625\n",
      "Epoch: 416, Loss: 24838.650390625\n",
      "Epoch: 417, Loss: 23255.41015625\n",
      "Epoch: 418, Loss: 23855.990234375\n",
      "Epoch: 419, Loss: 24737.30078125\n",
      "Epoch: 420, Loss: 24075.95703125\n",
      "Epoch: 421, Loss: 24118.69140625\n",
      "Epoch: 422, Loss: 23560.71875\n",
      "Epoch: 423, Loss: 24569.435546875\n",
      "Epoch: 424, Loss: 24339.5703125\n",
      "Epoch: 425, Loss: 24730.298828125\n",
      "Epoch: 426, Loss: 23513.640625\n",
      "Epoch: 427, Loss: 24361.5234375\n",
      "Epoch: 428, Loss: 24150.09375\n",
      "Epoch: 429, Loss: 24215.232421875\n",
      "Epoch: 430, Loss: 24887.505859375\n",
      "Epoch: 431, Loss: 24256.767578125\n",
      "Epoch: 432, Loss: 24022.572265625\n",
      "Epoch: 433, Loss: 26183.880859375\n",
      "Epoch: 434, Loss: 23801.154296875\n",
      "Epoch: 435, Loss: 25318.16796875\n",
      "Epoch: 436, Loss: 26257.23046875\n",
      "Epoch: 437, Loss: 23743.171875\n",
      "Epoch: 438, Loss: 23761.646484375\n",
      "Epoch: 439, Loss: 24453.54296875\n",
      "Epoch: 440, Loss: 23575.330078125\n",
      "Epoch: 441, Loss: 23745.767578125\n",
      "Epoch: 442, Loss: 23634.041015625\n",
      "Epoch: 443, Loss: 23773.578125\n",
      "Epoch: 444, Loss: 24973.998046875\n",
      "Epoch: 445, Loss: 24187.373046875\n",
      "Epoch: 446, Loss: 23635.310546875\n",
      "Epoch: 447, Loss: 24520.87890625\n",
      "Epoch: 448, Loss: 24066.375\n",
      "Epoch: 449, Loss: 24126.794921875\n",
      "Epoch: 450, Loss: 25897.71484375\n",
      "Epoch: 451, Loss: 25581.2890625\n",
      "Epoch: 452, Loss: 23698.634765625\n",
      "Epoch: 453, Loss: 24320.33984375\n",
      "Epoch: 454, Loss: 23149.326171875\n",
      "Epoch: 455, Loss: 23288.6171875\n",
      "Epoch: 456, Loss: 24297.873046875\n",
      "Epoch: 457, Loss: 24041.501953125\n",
      "Epoch: 458, Loss: 27751.0\n",
      "Epoch: 459, Loss: 25913.599609375\n",
      "Epoch: 460, Loss: 25178.33203125\n",
      "Epoch: 461, Loss: 26550.61328125\n",
      "Epoch: 462, Loss: 25371.798828125\n",
      "Epoch: 463, Loss: 26510.9140625\n",
      "Epoch: 464, Loss: 26017.099609375\n",
      "Epoch: 465, Loss: 24820.625\n",
      "Epoch: 466, Loss: 25556.056640625\n",
      "Epoch: 467, Loss: 25762.869140625\n",
      "Epoch: 468, Loss: 25747.451171875\n",
      "Epoch: 469, Loss: 25432.896484375\n",
      "Epoch: 470, Loss: 26343.544921875\n",
      "Epoch: 471, Loss: 24515.45703125\n",
      "Epoch: 472, Loss: 25849.962890625\n",
      "Epoch: 473, Loss: 26103.046875\n",
      "Epoch: 474, Loss: 26248.01171875\n",
      "Epoch: 475, Loss: 25954.056640625\n",
      "Epoch: 476, Loss: 24653.47265625\n",
      "Epoch: 477, Loss: 25710.845703125\n",
      "Epoch: 478, Loss: 24513.037109375\n",
      "Epoch: 479, Loss: 25896.37890625\n",
      "Epoch: 480, Loss: 24014.419921875\n",
      "Epoch: 481, Loss: 26060.23828125\n",
      "Epoch: 482, Loss: 25806.3515625\n",
      "Epoch: 483, Loss: 26019.27734375\n",
      "Epoch: 484, Loss: 25923.681640625\n",
      "Epoch: 485, Loss: 24915.03515625\n",
      "Epoch: 486, Loss: 24035.701171875\n",
      "Epoch: 487, Loss: 24213.37890625\n",
      "Epoch: 488, Loss: 24693.064453125\n",
      "Epoch: 489, Loss: 23912.197265625\n",
      "Epoch: 490, Loss: 23887.59765625\n",
      "Epoch: 491, Loss: 26219.42578125\n",
      "Epoch: 492, Loss: 25654.181640625\n",
      "Epoch: 493, Loss: 24673.23828125\n",
      "Epoch: 494, Loss: 23656.783203125\n",
      "Epoch: 495, Loss: 23651.841796875\n",
      "Epoch: 496, Loss: 23277.84375\n",
      "Epoch: 497, Loss: 24469.4296875\n",
      "Epoch: 498, Loss: 23432.857421875\n",
      "Epoch: 499, Loss: 24015.40625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a 1D array, got an array with shape (1459, 1459)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SalePrice'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/pandas/core/frame.py:4261\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value, refs)\u001b[0m\n\u001b[1;32m   4260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4261\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   4263\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SalePrice'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m predictions_nn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mpredict(test_df)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m (predictions_catboost \u001b[38;5;241m+\u001b[39m predictions_nn) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmake_kaggle_submission_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bdint/80240793-Big-Data-Intelligence/bdint/data.py:23\u001b[0m, in \u001b[0;36mmake_kaggle_submission_file\u001b[0;34m(prediction, test_df)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prediction) \u001b[38;5;241m==\u001b[39m test_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mtest_df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 23\u001b[0m \u001b[43msubmission_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalePrice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m prediction\n\u001b[1;32m     24\u001b[0m submission_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/pandas/core/frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4091\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/pandas/core/frame.py:4314\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4311\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   4312\u001b[0m             refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/pandas/core/frame.py:4264\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value, refs)\u001b[0m\n\u001b[1;32m   4261\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   4263\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[0;32m-> 4264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iset_item_mgr(loc, value, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/bdint-AM14Fwwj-py3.11/lib/python3.11/site-packages/pandas/core/internals/managers.py:1328\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[0;34m(self, loc, item, value, refs)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1329\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a 1D array, got an array with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1330\u001b[0m         )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     value \u001b[38;5;241m=\u001b[39m ensure_block_shape(value, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a 1D array, got an array with shape (1459, 1459)"
     ]
    }
   ],
   "source": [
    "predictions_catboost = catboost.predict(test_df)\n",
    "nn = NN(hidden_size1=1028, hidden_size2=512, hidden_size3=256)\n",
    "nn.learn(\n",
    "    x_train_df=train_df.drop(columns=[\"SalePrice\"], inplace=False),\n",
    "    y_train_df=pd.DataFrame(train_df[\"SalePrice\"]),\n",
    "    epochs=500, learning_rate=0.5, weight_decay=0.1\n",
    ")\n",
    "predictions_nn = nn.predict(test_df).detach().numpy()\n",
    "predictions = (predictions_catboost + predictions_nn) / 2\n",
    "make_kaggle_submission_file(predictions, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
